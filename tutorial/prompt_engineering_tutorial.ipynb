{"cells":[{"cell_type":"markdown","metadata":{"id":"4kwRDN9r0Exd"},"source":["<a target=\"_blank\" href=\"https://colab.research.google.com/github/UpstageAI/cookbook/blob/main/cookbooks/upstage/Solar-Full-Stack LLM-101/02_prompt_engineering.ipynb\">\n","<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"BY4MPpJQ0Exj"},"source":["# Prompt Engineering\n","\n","## Overview  \n","In this exercise, we will explore prompt engineering within the Solar framework. Prompt engineering is a crucial technique in leveraging large language models effectively by crafting prompts that elicit the desired responses from the model. This tutorial will walk you through various strategies for creating and refining prompts to optimize the performance of Solar in different NLP tasks.\n","\n","## Purpose of the Exercise\n","The purpose of this exercise is to equip users with practical skills in prompt engineering. By the end of this tutorial, users will be able to design effective prompts, understand the impact of prompt variations, and enhance the accuracy and relevance of responses generated by the Solar LLM. This foundational knowledge is essential for advanced applications and fine-tuning of language models.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"vaQiU0Qd0Exl","executionInfo":{"status":"ok","timestamp":1762494373272,"user_tz":-540,"elapsed":13179,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}}},"outputs":[],"source":["! pip install -qU langchain-upstage python-dotenv"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"nhF4goyA0Exp","executionInfo":{"status":"ok","timestamp":1762494550634,"user_tz":-540,"elapsed":526,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}}},"outputs":[],"source":["# @title set API key\n","# First, enroll your API key as the colab key.\n","from pprint import pprint\n","import os\n","\n","import warnings\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","from IPython import get_ipython\n","\n","upstage_api_key_env_name = \"upstage_api_key\"\n","\n","\n","def load_env():\n","    if \"google.colab\" in str(get_ipython()):\n","        # Running in Google Colab\n","        from google.colab import userdata\n","\n","        upstage_api_key = userdata.get(upstage_api_key_env_name)\n","        # print(upstage_api_key)\n","        return os.environ.setdefault(upstage_api_key_env_name, upstage_api_key)\n","    else:\n","        # Running in local Jupyter Notebook\n","        from dotenv import load_dotenv\n","\n","        load_dotenv()\n","        return os.environ.get(upstage_api_key_env_name)\n","\n","\n","UPSTAGE_API_KEY = load_env() # Setting API Key"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9D2GUZen0Exp","executionInfo":{"status":"ok","timestamp":1762494553464,"user_tz":-540,"elapsed":1317,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"e2975e58-73b7-4f96-dedd-cac4756308ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 16, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'solar-mini-250422', 'system_fingerprint': None, 'id': '9b312838-8b63-4bdd-918c-8ec60f68e521', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--f3f156c7-fe30-4eea-90e5-93a9d62d4bf3-0' usage_metadata={'input_tokens': 16, 'output_tokens': 8, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n","The capital of France is Paris.\n"]}],"source":["# Quick hello world\n","from langchain_upstage import ChatUpstage\n","\n","llm = ChatUpstage(api_key=UPSTAGE_API_KEY)\n","response = llm.invoke(\"What is the capital of France\")\n","\n","print(response)\n","print(response.content)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9afImTtI0Exr","executionInfo":{"status":"ok","timestamp":1762494581253,"user_tz":-540,"elapsed":1219,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"e408bee1-4221-41b5-dbd3-78631819bc8e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Answer: It's Seoul!!\n"]}],"source":["# Chat prompt\n","from langchain_core.prompts import ChatPromptTemplate\n","\n","chat_prompt = ChatPromptTemplate.from_messages(\n","    [\n","        (\"system\", \"You are a helpful assistant.\"),\n","        (\"human\", \"What is the capital of France?\"),\n","        (\"ai\", \"Answer: It's Paris!!\"),\n","        (\"human\", \"What about Korea?\"),\n","    ]\n",")\n","\n","# 3. define chain\n","from langchain_core.output_parsers import StrOutputParser\n","\n","chain = chat_prompt | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17jr8Fsg0Ext","executionInfo":{"status":"ok","timestamp":1762494660987,"user_tz":-540,"elapsed":2346,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"180c3905-549e-449e-b609-c06be88ab9bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["To solve this problem, let's break it down step by step:\n","\n","1. The cafeteria initially had 23 apples.\n","2. They used 20 apples to make lunch, so we subtract those from the initial amount:\n","   \\[\n","   23 - 20 = 3\n","   \\]\n","   After making lunch, they have 3 apples left.\n","\n","3. They then bought 6 more apples, so we add those to the remaining apples:\n","   \\[\n","   3 + 6 = 9\n","   \\]\n","\n","Therefore, the cafeteria has 9 apples in total.\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","Q: The cafeteria had 23 apples.\n","If they used 20 to make lunch and bought 6 more,\n","how many apples do they have?\n","\n","A: the answer is\n","\"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qm2WrVP00Ext","executionInfo":{"status":"ok","timestamp":1762494678912,"user_tz":-540,"elapsed":2752,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"4438f997-b04a-4d00-ceed-01c0c2171f2a"},"outputs":[{"output_type":"stream","name":"stdout","text":["To solve this problem, we need to calculate the total number of apples the cafeteria has after using some for lunch and buying more.\n","\n","1. The cafeteria started with 23 apples.\n","2. They used 20 apples to make lunch, so we subtract those from the initial amount:  \n","   \\( 23 - 20 = 3 \\) apples remaining.\n","3. They then bought 6 more apples, so we add those to the remaining apples:  \n","   \\( 3 + 6 = 9 \\) apples.\n","\n","Therefore, the cafeteria has 9 apples now.\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n","\n","A: The answer is 11.\n","\n","Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n","\n","A: the answer is\n","\"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"jLYJnY5S0Exu"},"source":["![CoT](figures/cot.webp)\n","\n","from https://arxiv.org/abs/2201.11903"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ia4K0Hl0Exu","executionInfo":{"status":"ok","timestamp":1762494685870,"user_tz":-540,"elapsed":1817,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"48af4353-8488-477d-9d1f-ce512a5a3650"},"outputs":[{"output_type":"stream","name":"stdout","text":["The cafeteria started with 23 apples. They used 20 to make lunch, so they had 23 - 20 = 3 apples left. Then they bought 6 more apples, so they have 3 + 6 = 9 apples now. The answer is 9.\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n","\n","A: Roger started with 5 balls. 2 cans of 3 tennis balls\n","each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n","\n","Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"Oau6WK7m0Exv"},"source":["![Zero-Shot COT](figures/zero-cot.webp)\n","\n","From https://arxiv.org/abs/2205.11916"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNkP3PaQ0Exv","executionInfo":{"status":"ok","timestamp":1762494695905,"user_tz":-540,"elapsed":2102,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"c8439faa-1cee-4133-aef7-09d6f0bbd25e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Step 1: The cafeteria initially had 23 apples.\n","\n","Step 2: They used 20 apples to make lunch, so we subtract 20 from 23.\n","23 - 20 = 3\n","\n","Step 3: They then bought 6 more apples, so we add 6 to the remaining apples.\n","3 + 6 = 9\n","\n","Therefore, the cafeteria now has 9 apples.\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n","\n","A: The answer is 11.\n","\n","Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\n","\n","A: Let's think step by step.\n","\"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"ifgeqSYK0Exw"},"source":["## divide and conquer\n","![divideandconquer](figures/divideandconquer.png)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ve6bx09b0Exw","executionInfo":{"status":"ok","timestamp":1762494798791,"user_tz":-540,"elapsed":1946,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"7f93c3ec-6f6e-483c-9050-9ea4a092edf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["1. What is the name of the large language model introduced in the text, and how many parameters does it have?\n","2. What is the method for scaling LLMs called, and how does it differ from other LLM up-scaling methods?\n","3. What is the name of the variant of the SOLAR model that has been fine-tuned for instruction-following capabilities, and how does it compare to Mixtral-8x7B-Instruct?\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","    Please provide three questions from the following text:\n","    ---\n","    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters,\n","    demonstrating superior performance in various natural language processing (NLP) tasks.\n","    Inspired by recent efforts to efficiently up-scale LLMs,\n","    we present a method for scaling LLMs called depth up-scaling (DUS),\n","    which encompasses depthwise scaling and continued pretraining.\n","    In contrast to other LLM up-scaling methods that use mixture-of-experts,\n","    DUS does not require complex changes to train and inference efficiently.\n","    We show experimentally that DUS is simple yet effective\n","    in scaling up high-performance LLMs from small ones.\n","    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\n","    a variant fine-tuned for instruction-following capabilities,\n","    surpassing Mixtral-8x7B-Instruct.\n","    SOLAR 10.7B is publicly available under the Apache 2.0 license,\n","    promoting broad access and application in the LLM field.\n","    \"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"107tgMnc0Exx","executionInfo":{"status":"ok","timestamp":1762494807582,"user_tz":-540,"elapsed":1309,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"5fbb13f0-9e68-4eb8-b05c-45d718eed941"},"outputs":[{"output_type":"stream","name":"stdout","text":["1. Large Language Model (LLM)\n","2. Depth Up-scaling (DUS)\n","3. Instruction-following capabilities\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","    Please extract three keywords from the following text:\n","    ---\n","    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters,\n","    demonstrating superior performance in various natural language processing (NLP) tasks.\n","    Inspired by recent efforts to efficiently up-scale LLMs,\n","    we present a method for scaling LLMs called depth up-scaling (DUS),\n","    which encompasses depthwise scaling and continued pretraining.\n","    In contrast to other LLM up-scaling methods that use mixture-of-experts,\n","    DUS does not require complex changes to train and inference efficiently.\n","    We show experimentally that DUS is simple yet effective\n","    in scaling up high-performance LLMs from small ones.\n","    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\n","    a variant fine-tuned for instruction-following capabilities,\n","    surpassing Mixtral-8x7B-Instruct.\n","    SOLAR 10.7B is publicly available under the Apache 2.0 license,\n","    promoting broad access and application in the LLM field.\n","    \"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zGREtjRQ0Exx","executionInfo":{"status":"ok","timestamp":1762494829544,"user_tz":-540,"elapsed":1360,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"3a23f77d-bcab-4be4-be9f-7cf15e5d5814"},"outputs":[{"output_type":"stream","name":"stdout","text":["What is the method for scaling large language models (LLMs) introduced in the text, and how does it differ from other LLM up-scaling methods?\n"]}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","    Please provide one question from the following text\n","    regarding \"Depth up-scaling (DUS)\":\n","\n","    ---\n","    We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters,\n","    demonstrating superior performance in various natural language processing (NLP) tasks.\n","    Inspired by recent efforts to efficiently up-scale LLMs,\n","    we present a method for scaling LLMs called depth up-scaling (DUS),\n","    which encompasses depthwise scaling and continued pretraining.\n","    In contrast to other LLM up-scaling methods that use mixture-of-experts,\n","    DUS does not require complex changes to train and inference efficiently.\n","    We show experimentally that DUS is simple yet effective\n","    in scaling up high-performance LLMs from small ones.\n","    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\n","    a variant fine-tuned for instruction-following capabilities,\n","    surpassing Mixtral-8x7B-Instruct.\n","    SOLAR 10.7B is publicly available under the Apache 2.0 license,\n","    promoting broad access and application in the LLM field.\n","    \"\"\"\n",")\n","chain = prompt_template | llm\n","response = chain.invoke({})\n","\n","print(response.content)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"5iAc3bN30Eyf","executionInfo":{"status":"ok","timestamp":1762495064601,"user_tz":-540,"elapsed":52,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"3aeaae11-325e-4587-ac53-47704667d499"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tell me a funny joke about chickens. Provide the joke only.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"Tell me a {adjective} joke about {content}. Provide the joke only.\"\n",")\n","prompt_template.format(adjective=\"funny\", content=\"chickens\")"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"TK9ZFLaL0Eyg","executionInfo":{"status":"ok","timestamp":1762495067839,"user_tz":-540,"elapsed":1323,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"7e0dfe3b-8536-4df4-a403-17854241a009"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Why don't chickens ever play cards in the jungle?\\n\\nBecause they're afraid of getting decked!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}],"source":["chain = prompt_template | llm | StrOutputParser()\n","chain.invoke({\"adjective\": \"funny\", \"content\": \"chickens\"})"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"L0KgG2s30Eyh","executionInfo":{"status":"ok","timestamp":1762495070114,"user_tz":-540,"elapsed":668,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"e7785f4f-e6b7-4870-99b6-83b7892d4f98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Why did the steak go to the doctor? Because it felt a little \"meaty\"!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}],"source":["chain.invoke({\"adjective\": \"funny\", \"content\": \"beef\"})"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"tDPGu5mQ0Eyh","executionInfo":{"status":"ok","timestamp":1762495077400,"user_tz":-540,"elapsed":1269,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"}},"outputId":"b1f08607-c4b9-4faa-c4c3-5b3affe225c0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'What is the name of the method for scaling large language models introduced in the text, which stands for depthwise scaling and continued pretraining?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}],"source":["from langchain_core.prompts import PromptTemplate\n","\n","prompt_template = PromptTemplate.from_template(\n","    \"\"\"\n","    Please provide one question from the following text\n","    regarding \"{keyword}\":\n","\n","    ---\n","    {text}\n","    \"\"\"\n",")\n","chain = prompt_template | llm | StrOutputParser()\n","keyword = \"DUS\"\n","text = \"\"\"\n","We introduce SOLAR 10.7B, a large language model (LLM) with 10.7 billion parameters,\n","    demonstrating superior performance in various natural language processing (NLP) tasks.\n","    Inspired by recent efforts to efficiently up-scale LLMs,\n","    we present a method for scaling LLMs called depth up-scaling (DUS),\n","    which encompasses depthwise scaling and continued pretraining.\n","    In contrast to other LLM up-scaling methods that use mixture-of-experts,\n","    DUS does not require complex changes to train and inference efficiently.\n","    We show experimentally that DUS is simple yet effective\n","    in scaling up high-performance LLMs from small ones.\n","    Building on the DUS model, we additionally present SOLAR 10.7B-Instruct,\n","    a variant fine-tuned for instruction-following capabilities,\n","    surpassing Mixtral-8x7B-Instruct.\n","    SOLAR 10.7B is publicly available under the Apache 2.0 license,\n","    promoting broad access and application in the LLM field.\n","\"\"\"\n","chain.invoke({\"keyword\": keyword, \"text\": text})"]},{"cell_type":"markdown","metadata":{"id":"qNDNS0NQ0Eyi"},"source":["# References\n","* https://platform.openai.com/docs/guides/prompt-engineering\n","* https://docs.anthropic.com/claude/docs/intro-to-prompting\n","* https://smith.langchain.com/hub"]}],"metadata":{"kernelspec":{"display_name":"nlp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}